{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "28i2aRdqye2U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb4c7904-0c5e-433c-8f03-d1585eca94e3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa soundfile"
      ],
      "metadata": {
        "id": "_DTyDXyIyhkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91fa4a66-5369-4b06-d03c-62dbd96f708c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.3)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.9.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lazy_loader>=0.1->librosa) (26.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.5.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (2.32.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2026.1.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy.fftpack import dct"
      ],
      "metadata": {
        "id": "7geF76yaylYZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label(filename):\n",
        "  # 0 - og\n",
        "  # 1 - spoofed\n",
        "    if \"_it\" in filename:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "UXMnWranyn7H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_RATE = 16000\n",
        "WINDOW_SEC = 3.0\n",
        "HOP_SEC = 1.0\n",
        "WINDOW_SAMPLES = int(WINDOW_SEC * SAMPLE_RATE)\n",
        "HOP_SAMPLES = int(HOP_SEC * SAMPLE_RATE)"
      ],
      "metadata": {
        "id": "sBbfHQKEyyKs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_logmel(y, sr):\n",
        "    mel = librosa.feature.melspectrogram(\n",
        "        y=y,\n",
        "        sr=sr,\n",
        "        n_fft=1024,\n",
        "        hop_length=256,\n",
        "        n_mels=80,\n",
        "        power=2.0\n",
        "    )\n",
        "    return librosa.power_to_db(mel, ref=np.max)"
      ],
      "metadata": {
        "id": "rfTRFnMMy5tL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_lfcc(y, sr, n_lfcc=40, n_fft=1024, hop_length=256, n_filters=128):\n",
        "    stft = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)\n",
        "    magnitude = np.abs(stft)\n",
        "    power_spectrum = magnitude ** 2\n",
        "\n",
        "    linear_filters = create_linear_filterbank(\n",
        "        n_filters=n_filters,\n",
        "        n_fft=n_fft,\n",
        "        sr=sr\n",
        "    )\n",
        "    filtered = np.dot(linear_filters, power_spectrum)\n",
        "    log_filtered = np.log(filtered + 1e-10)\n",
        "    lfcc = dct(log_filtered, type=2, axis=0, norm='ortho')[:n_lfcc]\n",
        "    lfcc = apply_cms(lfcc)\n",
        "\n",
        "    return lfcc\n",
        "\n",
        "\n",
        "def create_linear_filterbank(n_filters, n_fft, sr):\n",
        "\n",
        "    n_freq_bins = n_fft // 2 + 1\n",
        "    freq_bins = np.linspace(0, sr / 2, n_freq_bins)\n",
        "    center_freqs = np.linspace(0, sr / 2, n_filters + 2)\n",
        "    filterbank = np.zeros((n_filters, n_freq_bins))\n",
        "\n",
        "    for i in range(n_filters):\n",
        "        left = center_freqs[i]\n",
        "        center = center_freqs[i + 1]\n",
        "        right = center_freqs[i + 2]\n",
        "\n",
        "        rising_mask = (freq_bins >= left) & (freq_bins <= center)\n",
        "        filterbank[i, rising_mask] = (freq_bins[rising_mask] - left) / (center - left)\n",
        "\n",
        "        falling_mask = (freq_bins >= center) & (freq_bins <= right)\n",
        "        filterbank[i, falling_mask] = (right - freq_bins[falling_mask]) / (right - center)\n",
        "\n",
        "    return filterbank\n",
        "\n",
        "def apply_cms(features):\n",
        "    mean = np.mean(features, axis=1, keepdims=True)\n",
        "    return features - mean"
      ],
      "metadata": {
        "id": "obKAw_0fFTZN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.samples = []\n",
        "        for fname in os.listdir(root_dir):\n",
        "            if not fname.lower().endswith('.wav'):\n",
        "                continue\n",
        "\n",
        "            path = os.path.join(root_dir, fname)\n",
        "            label = get_label(fname)\n",
        "\n",
        "            y, sr = librosa.load(path, sr=SAMPLE_RATE)\n",
        "\n",
        "            for start in range(0, len(y) - WINDOW_SAMPLES + 1, HOP_SAMPLES):\n",
        "                chunk = y[start:start + WINDOW_SAMPLES]\n",
        "\n",
        "                self.samples.append((chunk, label, fname))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        chunk, label, fname = self.samples[idx]\n",
        "\n",
        "        lfcc = extract_lfcc(chunk, SAMPLE_RATE, n_lfcc=40)\n",
        "        lfcc = torch.tensor(lfcc).unsqueeze(0)  # [1, 40, T]\n",
        "\n",
        "        return (\n",
        "            lfcc.float(),\n",
        "            torch.tensor(label).long(),\n",
        "            fname\n",
        "        )"
      ],
      "metadata": {
        "id": "AJGQivdVzAmA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content/drive/MyDrive/audio_wav\"\n",
        "train_ds = ReplayDataset(os.path.join(base_path, \"train\"))\n",
        "val_ds   = ReplayDataset(os.path.join(base_path, \"valid\"))\n",
        "test_ds  = ReplayDataset(os.path.join(base_path, \"test\"))"
      ],
      "metadata": {
        "id": "er2fSHdvzQBQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = DataLoader(train_ds, batch_size=32, shuffle=True) #??\n",
        "val   = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
        "test  = DataLoader(test_ds, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "wXQnZMvbzXiM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "d2Fo5rlezf5E"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MFM(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels * 2,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=stride,\n",
        "            padding=padding\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        # Split channels into two halves\n",
        "        c = x.shape[1] // 2\n",
        "        x1, x2 = x[:, :c, :, :], x[:, c:, :, :]\n",
        "        return torch.max(x1, x2)"
      ],
      "metadata": {
        "id": "U-nU5yo-2L5R"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LCNNBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, padding=1):\n",
        "        super().__init__()\n",
        "        self.mfm = MFM(in_ch, out_ch, kernel_size, stride, padding)\n",
        "        self.bn = nn.BatchNorm2d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mfm(x)\n",
        "        x = self.bn(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "-od99dBF2bdj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LCNN(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            LCNNBlock(1, 32),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            LCNNBlock(32, 64),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            LCNNBlock(64, 128),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            LCNNBlock(128, 256),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.global_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "JvCHF30H2cq9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = LCNN(num_classes=2).to(device)"
      ],
      "metadata": {
        "id": "uwnfamkF2sPq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=1e-4,\n",
        "    weight_decay=1e-5\n",
        ")"
      ],
      "metadata": {
        "id": "o50cImgT33pF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy = torch.randn(4, 1, 80, 300).to(device)\n",
        "out = model(dummy)\n",
        "print(out.shape) #[4, 2]"
      ],
      "metadata": {
        "id": "rd91RSX-35yk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e81a1e46-6627-4d0d-cfc7-127296457451"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for mel, label, _ in loader:\n",
        "        mel = mel.to(device)       # (B, 1, 80, T)\n",
        "        label = label.to(device)   # (B,)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(mel)\n",
        "        loss = criterion(logits, label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * mel.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == label).sum().item()\n",
        "        total += label.size(0)\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    acc = correct / total\n",
        "\n",
        "    return avg_loss, acc\n"
      ],
      "metadata": {
        "id": "qVdf7rC83_Y2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for mel, label, _ in loader:\n",
        "        mel = mel.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        logits = model(mel)\n",
        "        loss = criterion(logits, label)\n",
        "\n",
        "        running_loss += loss.item() * mel.size(0)\n",
        "        _, preds = torch.max(logits, 1)\n",
        "        correct += (preds == label).sum().item()\n",
        "        total += label.size(0)\n",
        "\n",
        "    return running_loss / total, correct / total"
      ],
      "metadata": {
        "id": "xk6rUivx5lUu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "best_val_acc = 0.0\n",
        "\n",
        "patience = 5\n",
        "counter = 0\n",
        "min_delta = 1e-4\n",
        "\n",
        "SAVE_PATH = \"/content/drive/MyDrive/best_lcnn_model.pth\"\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_one_epoch(\n",
        "        model, train, optimizer, criterion, device\n",
        "    )\n",
        "\n",
        "    val_loss, val_acc = evaluate(\n",
        "        model, val, criterion, device\n",
        "    )\n",
        "\n",
        "    if val_acc > best_val_acc + min_delta:\n",
        "        best_val_acc = val_acc\n",
        "        counter = 0   # reset patience counter\n",
        "        torch.save(model.state_dict(), SAVE_PATH)\n",
        "        print(f\"Best model saved at epoch {epoch+1}\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"No improvement for {counter}/{patience} epochs\")\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch+1}/{num_epochs} | \"\n",
        "        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "        f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"\n",
        "    )\n",
        "\n",
        "    if counter >= patience:\n",
        "        print(\"\\nEarly stopping triggered.\")\n",
        "        break\n",
        "\n",
        "print(f\"\\nBest Val Acc: {best_val_acc:.4f}\")\n",
        "print(f\"Best model saved to: {SAVE_PATH}\")\n"
      ],
      "metadata": {
        "id": "lrFVWrH65yhv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2040e858-6414-4ebd-ad5a-90c0148fb032"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved at epoch 1\n",
            "Epoch 1/20 | Train Loss: 0.1883, Train Acc: 0.9305 | Val Loss: 0.0906, Val Acc: 0.9477\n",
            "Best model saved at epoch 2\n",
            "Epoch 2/20 | Train Loss: 0.0505, Train Acc: 0.9865 | Val Loss: 0.0332, Val Acc: 0.9869\n",
            "Best model saved at epoch 3\n",
            "Epoch 3/20 | Train Loss: 0.0362, Train Acc: 0.9891 | Val Loss: 0.0348, Val Acc: 0.9935\n",
            "No improvement for 1/5 epochs\n",
            "Epoch 4/20 | Train Loss: 0.0189, Train Acc: 0.9957 | Val Loss: 0.0437, Val Acc: 0.9771\n",
            "No improvement for 2/5 epochs\n",
            "Epoch 5/20 | Train Loss: 0.0156, Train Acc: 0.9954 | Val Loss: 0.0476, Val Acc: 0.9804\n",
            "No improvement for 3/5 epochs\n",
            "Epoch 6/20 | Train Loss: 0.0124, Train Acc: 0.9974 | Val Loss: 0.0205, Val Acc: 0.9935\n",
            "No improvement for 4/5 epochs\n",
            "Epoch 7/20 | Train Loss: 0.0081, Train Acc: 0.9984 | Val Loss: 0.0255, Val Acc: 0.9935\n",
            "No improvement for 5/5 epochs\n",
            "Epoch 8/20 | Train Loss: 0.0039, Train Acc: 1.0000 | Val Loss: 0.0297, Val Acc: 0.9869\n",
            "\n",
            "Early stopping triggered.\n",
            "\n",
            "Best Val Acc: 0.9935\n",
            "Best model saved to: /content/drive/MyDrive/best_lcnn_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "AAtGtNi153T_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/best_lcnn_model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "file_probs = defaultdict(list)   # file_id -> list of spoof probs\n",
        "file_labels = {}                 # file_id -> true label\n",
        "\n",
        "with torch.no_grad():\n",
        "    for mels, labels, file_ids in test:\n",
        "        mels = mels.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        logits = model(mels)              # (B, 2)\n",
        "        probs = F.softmax(logits, dim=1)  # (B, 2)\n",
        "        spoof_probs = probs[:, 1]         # class-1 = spoof\n",
        "\n",
        "        for i, fid in enumerate(file_ids):\n",
        "            file_probs[fid].append(spoof_probs[i].item())\n",
        "            file_labels[fid] = labels[i].item()"
      ],
      "metadata": {
        "id": "TzcurEAc_BWp"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = []\n",
        "y_score = []\n",
        "\n",
        "for fid in file_probs:\n",
        "    y_true.append(file_labels[fid])\n",
        "    y_score.append(np.mean(file_probs[fid]))  # mean over chunks\n",
        "\n",
        "y_true = np.array(y_true)\n",
        "y_score = np.array(y_score)\n"
      ],
      "metadata": {
        "id": "QdDarDs3_EzG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve\n",
        "file_ids = list(file_probs.keys())\n",
        "\n",
        "y_score = np.array([np.mean(file_probs[fid]) for fid in file_ids])\n",
        "y_true  = np.array([file_labels[fid] for fid in file_ids])  # 0=bonafide, 1=spoof\n",
        "\n",
        "threshold = 0.5\n",
        "y_pred = (y_score >= threshold).astype(int)\n",
        "\n",
        "file_acc = (y_pred == y_true).mean()\n",
        "file_error = 1.0 - file_acc\n",
        "\n",
        "print(f\"File-level Accuracy: {file_acc:.4f}\")\n",
        "print(f\"File-level Error Rate: {file_error:.4f}\")\n",
        "\n",
        "TP = np.sum((y_pred == 1) & (y_true == 1))  # spoof → spoof\n",
        "TN = np.sum((y_pred == 0) & (y_true == 0))  # bonafide → bonafide\n",
        "FP = np.sum((y_pred == 1) & (y_true == 0))  # bonafide → spoof\n",
        "FN = np.sum((y_pred == 0) & (y_true == 1))  # spoof → bonafide\n",
        "FAR = FP / (FP + TN + 1e-8)\n",
        "FRR = FN / (FN + TP + 1e-8)\n",
        "\n",
        "print(f\"\\nFAR (False Accept Rate): {FAR:.4f}\")\n",
        "print(f\"FRR (False Reject Rate): {FRR:.4f}\")\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=1)\n",
        "fnr = 1 - tpr\n",
        "\n",
        "eer_idx = np.argmin(np.abs(fpr - fnr))\n",
        "eer = (fpr[eer_idx] + fnr[eer_idx]) / 2\n",
        "eer_threshold = thresholds[eer_idx]\n",
        "\n",
        "print(f\"\\nEER: {eer*100:.2f}%\")\n",
        "print(f\"EER Threshold: {eer_threshold:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkCuje7nNiMu",
        "outputId": "473de5f3-2f65-40ce-a287-64dc814bb9cb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File-level Accuracy: 1.0000\n",
            "File-level Error Rate: 0.0000\n",
            "\n",
            "FAR (False Accept Rate): 0.0000\n",
            "FRR (False Reject Rate): 0.0000\n",
            "\n",
            "EER: 0.00%\n",
            "EER Threshold: 0.9479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_original_id(filename):\n",
        "    name = filename.split(\".\")[0]\n",
        "    return name.split(\"_it\")[0] + \".wav\"\n"
      ],
      "metadata": {
        "id": "2-T4JAjSPs8-"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_originals = set()\n",
        "\n",
        "for _, _, file_ids in train:\n",
        "    for fid in file_ids:\n",
        "        train_originals.add(get_original_id(fid))\n"
      ],
      "metadata": {
        "id": "Yjn25NEQPuCp"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_originals = set()\n",
        "\n",
        "for _, _, file_ids in test:\n",
        "    for fid in file_ids:\n",
        "        test_originals.add(get_original_id(fid))\n"
      ],
      "metadata": {
        "id": "ZLukgmzTPxTg"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "leakage = train_originals & test_originals\n",
        "\n",
        "print(f\"Number of overlapping original files: {len(leakage)}\")\n",
        "print(leakage)\n"
      ],
      "metadata": {
        "id": "ApV4IPGuP6z8",
        "outputId": "58742135-6950-418a-fc70-fd03b8cce446",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of overlapping original files: 0\n",
            "set()\n"
          ]
        }
      ]
    }
  ]
}